{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "81c72b30",
            "metadata": {},
            "source": [
                "# Clone full repo to copy aditional python files if running on colab"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This cell download all the aditional pythons files that are used in the notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "af0f0e27",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Only run if in colab\n",
                "!RunningInCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
                "# Continue only if running on Google Colab\n",
                "![[ ! RunningInCOLAB ]] && exit\n",
                "\n",
                "# clone repo and move to current working dir\n",
                "!git clone https://github.com/evaluating-effectiveness-cloud-nlp/replication_package.git repo\n",
                "!rsync -av repo/ .\n",
                "!rm -rf repo"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18c39734",
            "metadata": {},
            "source": [
                "# Installing dependencies with pip"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Install dependencies using the file `requirements.txt` downloaded from previous cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "26aae54b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# installs dependencies\n",
                "%pip install -r requirements.txt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Providers Credentials"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Insert here Azure (Microsoft), AWS (Amazon) and Google credentials. For Google is needed to obtain the `google-credentials.json` with all credentials and upload it when the cell executes.\n",
                "This cells will create an `credentials.py` file in the same directory of the *notebook*.\n",
                "\n",
                "> Note: If you don't have credentials and just want to test the experiment, you can run the experiment using mock providers with random outputs (see section \"Importing or Mocking MLaaS providers\")."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from google.colab import files\n",
                "\n",
                "# @markdown Microsoft\n",
                "azure_key_1 = '6565' # @param {type:\"string\"}\n",
                "azure_key_2 = '656565' # @param {type:\"string\"}\n",
                "azure_location = '6565656' # @param {type:\"string\"}\n",
                "azure_endpoint = '6565656' # @param {type:\"string\"}\n",
                "\n",
                "# @markdown Amazon\n",
                "aws_access_key_id='6565656' # @param {type:\"string\"}\n",
                "aws_secret_access_key='46556' # @param {type:\"string\"}\n",
                "\n",
                "\n",
                "# @markdown Google\n",
                "# @markdown >*You will need to upload `google-credentials.json` file on runtime as described in: https://developers.google.com/workspace/guides/create-credentials?hl=pt-br#create_credentials_for_a_service_account*\n",
                "print('Please upload \"google-credentials.json\" file')\n",
                "google_credentials_file = files.upload()\n",
                "\n",
                "file_name = list(google_credentials_file.keys())[0]\n",
                "\n",
                "## write credentials to a credentials.py file\n",
                "file_content = f\"\"\"\n",
                "import os\n",
                "\n",
                "# Microsoft\n",
                "azure_key_1 = '{azure_key_1}'\n",
                "azure_key_2 = '{azure_key_2}'\n",
                "azure_location = '{azure_location}'\n",
                "azure_endpoint = '{azure_endpoint}'\n",
                "\n",
                "# Amazon\n",
                "aws_access_key_id = '{aws_access_key_id}'\n",
                "aws_secret_access_key = '{aws_secret_access_key}'\n",
                "\n",
                "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./{file_name}\"\n",
                "\"\"\"\n",
                "\n",
                "file_name = \"credentials.py\"\n",
                "\n",
                "# Escreve o conteúdo no arquivo\n",
                "with open(file_name, \"w\") as f:\n",
                "    f.write(file_content)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73c8c2b4-7d16-4424-a90c-7ed7bfb8b205",
            "metadata": {},
            "source": [
                "## Download the pre-trained ``glove.twitter`` word embedding model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This *wordembedding* model is only used in the noise WordEmbeddings.\n",
                "If you don't want to use it, just remove it from the noise list and don't run this cell.\n",
                "> Note: The file has been placed in a personal repository just for ease of download, the original model is available as a *.zip file at: https://github.com/stanfordnlp/GloVe"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "8d410856-5d7f-431c-af3e-ab89fb7df372",
            "metadata": {
                "tags": [
                    "common"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: ipywidgets in /home/anonymoususer/.local/lib/python3.8/site-packages (7.7.2)\n",
                        "Requirement already satisfied: ipykernel>=4.5.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipywidgets) (5.5.6)\n",
                        "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
                        "Requirement already satisfied: traitlets>=4.3.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipywidgets) (5.13.0)\n",
                        "Requirement already satisfied: widgetsnbextension~=3.6.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipywidgets) (3.6.6)\n",
                        "Requirement already satisfied: ipython>=4.0.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipywidgets) (7.34.0)\n",
                        "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipywidgets) (1.1.7)\n",
                        "Requirement already satisfied: jupyter-client in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n",
                        "Requirement already satisfied: tornado>=4.2 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\n",
                        "Requirement already satisfied: setuptools>=18.5 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (68.2.2)\n",
                        "Requirement already satisfied: jedi>=0.16 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.19.1)\n",
                        "Requirement already satisfied: decorator in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.0.2)\n",
                        "Requirement already satisfied: pickleshare in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
                        "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.39)\n",
                        "Requirement already satisfied: pygments in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.16.1)\n",
                        "Requirement already satisfied: backcall in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
                        "Requirement already satisfied: matplotlib-inline in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
                        "Requirement already satisfied: pexpect>4.3 in /home/anonymoususer/.local/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
                        "Requirement already satisfied: notebook>=4.4.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.5)\n",
                        "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
                        "Requirement already satisfied: jinja2 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.2)\n",
                        "Requirement already satisfied: pyzmq<25,>=17 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (24.0.1)\n",
                        "Requirement already satisfied: argon2-cffi in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
                        "Requirement already satisfied: jupyter-core>=4.6.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.5.0)\n",
                        "Requirement already satisfied: nbformat in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.2)\n",
                        "Requirement already satisfied: nbconvert>=5 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.10.0)\n",
                        "Requirement already satisfied: nest-asyncio>=1.5 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.8)\n",
                        "Requirement already satisfied: Send2Trash>=1.8.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.2)\n",
                        "Requirement already satisfied: terminado>=0.8.3 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.17.1)\n",
                        "Requirement already satisfied: prometheus-client in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.0)\n",
                        "Requirement already satisfied: nbclassic>=0.4.7 in /home/anonymoususer/.local/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.6)\n",
                        "Requirement already satisfied: entrypoints in /home/anonymoususer/.local/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
                        "Requirement already satisfied: ptyprocess>=0.5 in /home/anonymoususer/.local/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
                        "Requirement already satisfied: wcwidth in /home/anonymoususer/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.9)\n",
                        "Requirement already satisfied: platformdirs>=2.5 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jupyter-core>=4.6.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.11.0)\n",
                        "Requirement already satisfied: jupyter-server>=1.8 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.24.0)\n",
                        "Requirement already satisfied: notebook-shim>=0.2.3 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.3)\n",
                        "Requirement already satisfied: beautifulsoup4 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.8.2)\n",
                        "Requirement already satisfied: bleach!=5.0.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.1.0)\n",
                        "Requirement already satisfied: defusedxml in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
                        "Requirement already satisfied: importlib-metadata>=3.6 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.8.0)\n",
                        "Requirement already satisfied: jupyterlab-pygments in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.2)\n",
                        "Requirement already satisfied: markupsafe>=2.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.3)\n",
                        "Requirement already satisfied: mistune<4,>=2.0.3 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
                        "Requirement already satisfied: nbclient>=0.5.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.0)\n",
                        "Requirement already satisfied: packaging in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.2)\n",
                        "Requirement already satisfied: pandocfilters>=1.4.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
                        "Requirement already satisfied: tinycss2 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.1)\n",
                        "Requirement already satisfied: fastjsonschema in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.18.1)\n",
                        "Requirement already satisfied: jsonschema>=2.6 in /home/anonymoususer/.local/lib/python3.8/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.19.2)\n",
                        "Requirement already satisfied: six>=1.5 in /home/anonymoususer/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
                        "Requirement already satisfied: argon2-cffi-bindings in /home/anonymoususer/.local/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
                        "Requirement already satisfied: webencodings in /home/anonymoususer/.local/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
                        "Requirement already satisfied: zipp>=0.5 in /home/anonymoususer/.local/lib/python3.8/site-packages (from importlib-metadata>=3.6->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.17.0)\n",
                        "Requirement already satisfied: attrs>=22.2.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
                        "Requirement already satisfied: importlib-resources>=1.4.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.9.0)\n",
                        "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2023.7.1)\n",
                        "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.10)\n",
                        "Requirement already satisfied: referencing>=0.28.4 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.30.2)\n",
                        "Requirement already satisfied: rpds-py>=0.7.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
                        "Requirement already satisfied: anyio<4,>=3.1.0 in /home/anonymoususer/.local/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.7.1)\n",
                        "Requirement already satisfied: websocket-client in /home/anonymoususer/.local/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.6.4)\n",
                        "Requirement already satisfied: cffi>=1.0.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
                        "Requirement already satisfied: soupsieve>=1.2 in /home/anonymoususer/.local/lib/python3.8/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.5)\n",
                        "Requirement already satisfied: idna>=2.8 in /home/anonymoususer/.local/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.4)\n",
                        "Requirement already satisfied: sniffio>=1.1 in /home/anonymoususer/.local/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
                        "Requirement already satisfied: exceptiongroup in /home/anonymoususer/.local/lib/python3.8/site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.3)\n",
                        "Requirement already satisfied: pycparser in /home/anonymoususer/.local/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
                        "file  models/glove.twitter.27B.100d.txt  already exists.\n"
                    ]
                }
            ],
            "source": [
                "!python -m pip install ipywidgets\n",
                "import urllib.request\n",
                "from os.path import exists\n",
                "import ipywidgets as widgets\n",
                "from IPython.display import display\n",
                "import os\n",
                "\n",
                "progress = None\n",
                "def show_progress(block_num, block_size, total_size):\n",
                "    global progress\n",
                "    if not progress :\n",
                "        progress = widgets.FloatProgress(\n",
                "            value=0,\n",
                "            min=0,\n",
                "            max=total_size,\n",
                "            step=0.1,\n",
                "            description='Downloading',\n",
                "            bar_style='info',\n",
                "            orientation='horizontal'\n",
                "        )\n",
                "        display(progress)\n",
                "        \n",
                "    downloaded = (block_num * block_size)\n",
                "    print(block_num * block_size, \"/\", total_size,\"\\r\", end=\"\")\n",
                "    \n",
                "    progress.value = downloaded\n",
                "\n",
                "model_path = \"models/glove.twitter.27B.100d.txt\"\n",
                "word_embedding_url = \"https://huggingface.co/anonymoususer/fault_injection_mlaas/resolve/main/glove.twitter.27B.100d.txt\"\n",
                "\n",
                "file_exists = exists(model_path)\n",
                "\n",
                "if file_exists :\n",
                "    print(\"file \", model_path, \" already exists.\")\n",
                "else:\n",
                "    filename = \"models\"\n",
                "    os.makedirs(filename, exist_ok=True)\n",
                "    urllib.request.urlretrieve(word_embedding_url, model_path, show_progress)\n",
                "    print(\"File downloaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b1e142e4-828c-42c5-9e9b-df0797ee7462",
            "metadata": {},
            "source": [
                "## Importing and Mocking MLaaS providers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This cell imports the module container all providers implementations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "755e5348-bbf8-452c-9801-3ced92238e27",
            "metadata": {},
            "outputs": [],
            "source": [
                "from mlaas_providers import providers as ml_providers"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "13fee9d3-c191-4629-805d-1ea45992965d",
            "metadata": {},
            "source": [
                "Run the cell below in addition to the previous one if you want to use simulated providers instead of real providers.\n",
                "> Note: This will return random values for sentiment analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "85da9cd7-5606-45fb-af7f-b76f30ded8d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "ml_providers.amazon = ml_providers.return_mock_of(ml_providers.amazon)\n",
                "ml_providers.google = ml_providers.return_mock_of(ml_providers.google)\n",
                "ml_providers.microsoft = ml_providers.return_mock_of(ml_providers.microsoft)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4bded844-ed3f-4355-a758-0491eb1f624e",
            "metadata": {
                "tags": []
            },
            "source": [
                "# `RQ1`: How effective are the Cloud NLP services when subjected to noise?"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9d46011a-b681-4645-abe3-cd8385908f3d",
            "metadata": {},
            "source": [
                "## Importing aditional python modules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "42a5089e-6025-4bcf-9260-22425a249776",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package omw-1.4 to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "from datetime import datetime\n",
                "from typing import List\n",
                "from mlaas_providers.providers import read_dataset\n",
                "from noise_insertion.utils import save_data_to_file\n",
                "from data_sampling.data_sampling import DataSampling\n",
                "from noise_insertion.percent_insertion import noises\n",
                "from noise_insertion import noise_insertion\n",
                "from visualization import visualization\n",
                "from progress import progress_manager\n",
                "from metrics import metrics\n",
                "import ipywidgets as widgets\n",
                "data_sampling = DataSampling()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6544ecda-802d-4cf9-9919-ed7800d8a5f6",
            "metadata": {},
            "source": [
                "## Parameters"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Choose the sample size, types of noise to be used and noise levels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "7cad8723-fe62-4ce3-8a45-68f9269ea53d",
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_size = 99\n",
                "\n",
                "noise_list =[\n",
                "    noises.Keyboard,\n",
                "    noises.OCR,\n",
                "    noises.RandomCharReplace,\n",
                "    noises.CharSwap,\n",
                "    noises.WordSwap,\n",
                "    noises.WordSplit,\n",
                "    noises.Antonym,\n",
                "    noises.Synonym,\n",
                "    noises.Spelling,\n",
                "    noises.TfIdfWord,\n",
                "    noises.WordEmbeddings,\n",
                "    noises.ContextualWordEmbs,\n",
                "]\n",
                "\n",
                "noise_level=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2e8c81e4-cfdb-4773-b0f9-90e6e6e326d2",
            "metadata": {},
            "source": [
                "## Running the experiment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**TODO: ADICIONAR UMA DESCRIÇÃO DO QUE É FEITO NESTA RQ1, TALVEZ CITAR O PIPELINE E SEÇÃO**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The RQ1 experiment raw results are stored in a new directory inside outputs/experiment1 folder.\n",
                "\n",
                "If for any reason an error occurs during execution, you can continue where you left off by entering the name of the directory created during execution below. Ex.: `size99_07-12-2022 09_34_29`."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "25c39d25-848c-4897-9179-d8309374cd38",
            "metadata": {},
            "source": [
                "To continue from previously ongoing progress insert the name of a /outputs/experiment1 folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "5f8fe718-b947-43cd-8aa2-0ffc21795c5c",
            "metadata": {
                "scrolled": true,
                "tags": [
                    "experiment1"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results will be stored at:  ./outputs/experiment1/size99_07-12-2022 09_34_29\n",
                        "Generating noise...\n",
                        "- Keyboard\n",
                        "-- \n",
                        "- OCR\n",
                        "-- \n",
                        "- RandomCharReplace\n",
                        "-- \n",
                        "- CharSwap\n",
                        "-- \n",
                        "- WordSwap\n",
                        "-- \n",
                        "- WordSplit\n",
                        "-- \n",
                        "- Antonym\n",
                        "-- \n",
                        "- Synonym\n",
                        "-- \n",
                        "- Spelling\n",
                        "-- \n",
                        "- TfIdfWord\n",
                        "-- \n",
                        "- WordEmbeddings\n",
                        "-- \n",
                        "- ContextualWordEmbs\n",
                        "-- \n",
                        "Getting predictions from providers...\n",
                        "- google\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- microsoft\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- amazon\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "Calculating metrics...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                        "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Results were saved to: ./outputs/experiment1/size99_07-12-2022 09_34_29\n"
                    ]
                }
            ],
            "source": [
                "# @markdown ### Type the name of a /outputs/experiment1 folder to continue from:\n",
                "continue_from = \"\" # @param {type:\"string\"}\n",
                "\n",
                "def get_main_path(size):\n",
                "    now = datetime.now()\n",
                "    timestamp = now.strftime(\"%m-%d-%Y %H_%M_%S\")\n",
                "    main_dir = './outputs/experiment1/size'+str(size)+'_' + timestamp\n",
                "    return main_dir\n",
                "\n",
                "def run_evaluation(sample_size: int,\n",
                "                  noise_levels: List[int] =[0.1, 0.15, 0.2, 0.25, 0.3],\n",
                "                  noise_algorithms=[noises.no_noise, noises.RandomCharReplace, noises.Keyboard, noises.OCR],\n",
                "                  mlaas_providers=[ml_providers.google],\n",
                "                  continue_from=None):\n",
                "    if(continue_from):\n",
                "        main_path = './outputs/experiment1/'+continue_from\n",
                "        progress = progress_manager.load_progress(main_path)\n",
                "        x_dataset = read_dataset(main_path + '/data' + \"/dataset.xlsx\")\n",
                "        y_labels = read_dataset(main_path + '/data' + \"/labels.xlsx\")\n",
                "    else:\n",
                "        x_dataset, y_labels = data_sampling.get_dataset_sample('./Tweets_dataset.csv', sample_size)\n",
                "        main_path = get_main_path(len(x_dataset))\n",
                "        save_data_to_file(x_dataset, main_path + '/data', \"dataset\")\n",
                "        save_data_to_file(y_labels, main_path + '/data', \"labels\")\n",
                "        \n",
                "        progress = progress_manager.init_progress(main_path, noise_algorithms, noise_levels, mlaas_providers)\n",
                "    print(\"Results will be stored at: \", main_path)\n",
                "    print('Generating noise...')\n",
                "    progress = noise_insertion.generate_noised_data(x_dataset, main_path)\n",
                "\n",
                "    print('Getting predictions from providers...')\n",
                "    progress = ml_providers.get_prediction_results(main_path)\n",
                "\n",
                "    print('Calculating metrics...')\n",
                "    metrics_results = metrics.metrics(progress, y_labels, main_path)\n",
                "\n",
                "    noise_list = [0.0]\n",
                "    noise_list.extend(noise_levels)\n",
                "\n",
                "    visualization.plot_results(metrics_results, main_path + '/results', noise_list)\n",
                "\n",
                "    print(\"Results were saved to:\", main_path)\n",
                "\n",
                "run_evaluation(\n",
                "    sample_size,\n",
                "    noise_levels=noise_level,\n",
                "    noise_algorithms=noise_list,\n",
                "    mlaas_providers=[ml_providers.amazon, ml_providers.microsoft, ml_providers.google],\n",
                "    continue_from=continue_from\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "68d35a8d-6295-4a38-9c70-37d5be28072f",
            "metadata": {},
            "source": [
                "# Experiment 2"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "df5f2c30-5614-4ca1-addc-1acb0ff15c32",
            "metadata": {},
            "source": [
                "## Importing aditional python modules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "86529d03-82f5-42c3-a82d-21483ea5245d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
                        "[nltk_data]       date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package omw-1.4 to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\rocha\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "from typing import TypedDict, List\n",
                "from datetime import datetime\n",
                "from data_sampling.data_sampling import DataSampling\n",
                "from progress import progress_manager\n",
                "from noise_insertion.percent_insertion import noises\n",
                "from noise_insertion import noise_insertion\n",
                "from mlaas_providers.providers import read_dataset\n",
                "from metrics import metrics\n",
                "from visualization import visualization"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "aa20e114-47bb-4e9f-9b5e-9bc96a78d716",
            "metadata": {},
            "source": [
                "## Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9b8e59c3-987e-4392-bc39-fd992c7cfbcb",
            "metadata": {},
            "outputs": [],
            "source": [
                "sample_size=100\n",
                "noise_level=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
                "\n",
                "word_counts = [\n",
                "    {\"min_width\": 12, \"max_width\": 12},\n",
                "    {\"min_width\": 19, \"max_width\": 19},\n",
                "    {\"min_width\": 23, \"max_width\": 23},\n",
                "]\n",
                "\n",
                "noise_algo = [\n",
                "    noises.Keyboard,\n",
                "    noises.OCR,\n",
                "    noises.RandomCharReplace,\n",
                "    noises.CharSwap,\n",
                "    noises.WordSwap,\n",
                "    noises.WordSplit,\n",
                "    noises.Antonym,\n",
                "    noises.Synonym,\n",
                "    noises.Spelling,\n",
                "    noises.TfIdfWord,\n",
                "    noises.WordEmbeddings,\n",
                "    noises.ContextualWordEmbs,\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5b701c73-2a43-49b6-8f21-c301298ad890",
            "metadata": {},
            "source": [
                "## Running"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bc0d83e1-3f97-40c2-86b0-527b081181c0",
            "metadata": {
                "tags": []
            },
            "source": [
                "To continue from previously ongoing progress insert the name of a /outputs/outputs/experiment2 folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "db0b580f-de61-4385-96bc-68e09867a0a7",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7aff8a23b07245ad99f7d136df001ecb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Textarea(value='', description='Continue from', placeholder='Type the name of a /outputs/experiment2 folder to…"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "import ipywidgets as widgets\n",
                "\n",
                "continue_widget = widgets.Textarea(\n",
                "    value='',\n",
                "    placeholder='Type the name of a /outputs/experiment2 folder to continue from',\n",
                "    description='Continue from',\n",
                "    disabled=False\n",
                ")\n",
                "continue_widget"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "631762a4-dabb-40ec-9cad-8a6edd63d9b1",
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "size100_12-21-2022 20_53_00\n",
                        "continue_from: ./outputs/experiment2/size100_12-21-2022 20_53_00\n",
                        "Generating noise...\n",
                        "- Keyboard\n",
                        "-- \n",
                        "- OCR\n",
                        "-- \n",
                        "- RandomCharReplace\n",
                        "-- \n",
                        "- CharSwap\n",
                        "-- \n",
                        "- WordSwap\n",
                        "-- \n",
                        "- WordSplit\n",
                        "-- \n",
                        "- Antonym\n",
                        "-- \n",
                        "- Synonym\n",
                        "-- \n",
                        "- Spelling\n",
                        "-- \n",
                        "- TfIdfWord\n",
                        "-- \n",
                        "- WordEmbeddings\n",
                        "-- \n",
                        "- ContextualWordEmbs\n",
                        "-- \n",
                        "Getting predictions from providers...\n",
                        "- google\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- amazon\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- microsoft\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "Calculating metrics...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                        "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "./outputs/experiment2/size100_12-21-2022 20_53_00/[12-12]\n",
                        "Generating noise...\n",
                        "- Keyboard\n",
                        "-- \n",
                        "- OCR\n",
                        "-- \n",
                        "- RandomCharReplace\n",
                        "-- \n",
                        "- CharSwap\n",
                        "-- \n",
                        "- WordSwap\n",
                        "-- \n",
                        "- WordSplit\n",
                        "-- \n",
                        "- Antonym\n",
                        "-- \n",
                        "- Synonym\n",
                        "-- \n",
                        "- Spelling\n",
                        "-- \n",
                        "- TfIdfWord\n",
                        "-- \n",
                        "- WordEmbeddings\n",
                        "-- \n",
                        "- ContextualWordEmbs\n",
                        "-- \n",
                        "Getting predictions from providers...\n",
                        "- google\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- amazon\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- microsoft\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "Calculating metrics...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                        "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "./outputs/experiment2/size100_12-21-2022 20_53_00/[19-19]\n",
                        "Generating noise...\n",
                        "- Keyboard\n",
                        "-- \n",
                        "- OCR\n",
                        "-- \n",
                        "- RandomCharReplace\n",
                        "-- \n",
                        "- CharSwap\n",
                        "-- \n",
                        "- WordSwap\n",
                        "-- \n",
                        "- WordSplit\n",
                        "-- \n",
                        "- Antonym\n",
                        "-- \n",
                        "- Synonym\n",
                        "-- \n",
                        "- Spelling\n",
                        "-- \n",
                        "- TfIdfWord\n",
                        "-- \n",
                        "- WordEmbeddings\n",
                        "-- \n",
                        "- ContextualWordEmbs\n",
                        "-- \n",
                        "Getting predictions from providers...\n",
                        "- google\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- amazon\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- \n",
                        "-- WordEmbeddings\n",
                        "--- \n",
                        "-- ContextualWordEmbs\n",
                        "--- \n",
                        "- microsoft\n",
                        "-- Keyboard\n",
                        "--- \n",
                        "-- OCR\n",
                        "--- \n",
                        "-- RandomCharReplace\n",
                        "--- \n",
                        "-- CharSwap\n",
                        "--- \n",
                        "-- WordSwap\n",
                        "--- \n",
                        "-- WordSplit\n",
                        "--- \n",
                        "-- Antonym\n",
                        "--- \n",
                        "-- Synonym\n",
                        "--- \n",
                        "-- Spelling\n",
                        "--- \n",
                        "-- TfIdfWord\n",
                        "--- 0.9 , \n",
                        "-- WordEmbeddings\n",
                        "--- 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , \n",
                        "-- ContextualWordEmbs\n",
                        "--- 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , \n",
                        "Calculating metrics...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\rocha\\repos\\ml_experiments\\fault_injection_mlaas\\visualization\\latex.py:107: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
                        "  df.loc[:, noise_column_name] = pd.to_numeric(df[noise_column_name])\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "./outputs/experiment2/size100_12-21-2022 20_53_00/[23-23]\n",
                        "['./outputs/experiment2/size100_12-21-2022 20_53_00/[12-12]', './outputs/experiment2/size100_12-21-2022 20_53_00/[19-19]', './outputs/experiment2/size100_12-21-2022 20_53_00/[23-23]']\n"
                    ]
                }
            ],
            "source": [
                "continue_from = continue_widget.value \n",
                "print(continue_from)\n",
                "class Size(TypedDict):\n",
                "    min_width: int\n",
                "    max_width: int\n",
                "    \n",
                "def create_main_path(timestamp, size):\n",
                "    main_dir = f'./outputs/experiment2/size{str(size)}_{timestamp}'\n",
                "\n",
                "    Path(main_dir).mkdir(parents=True, exist_ok=True)\n",
                "    return main_dir\n",
                "\n",
                "def create_sub_path(main_path: str, min_width: int, max_width: int):\n",
                "    path = f'{main_path}/[{str(min_width)}-{str(max_width)}]'\n",
                "    \n",
                "    Path(path).mkdir(parents=True, exist_ok=True)\n",
                "    Path(path+'/data').mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    return path\n",
                "\n",
                "def prepare_execution(\n",
                "    continue_from: str,\n",
                "    timestamp: str,\n",
                "    sample_size: int,\n",
                "    sizes: List[Size],\n",
                "    noise_algorithms,\n",
                "    noise_levels,\n",
                "    mlaas_providers\n",
                "):\n",
                "    dataSampling = DataSampling()\n",
                "    if not len(continue_from) > 0:\n",
                "        main_path = create_main_path(timestamp, sample_size)\n",
                "    else:\n",
                "        continue_from = './outputs/experiment2/'+ continue_from\n",
                "        print(\"continue_from:\", continue_from)\n",
                "        main_path = continue_from\n",
                "    \n",
                "    sub_path_list = []\n",
                "    for size in sizes:\n",
                "        min_width = size['min_width']\n",
                "        max_width = size['max_width']\n",
                "        sub_path = create_sub_path(main_path, min_width, max_width)\n",
                "\n",
                "        data, labels = dataSampling.get_by_word_count('Tweets_dataset.csv',\n",
                "                                              sample_size,\n",
                "                                              min_width,\n",
                "                                              max_width)\n",
                "\n",
                "        path = Path(sub_path+\"/data/dataset.xlsx\")\n",
                "        if not path.is_file():\n",
                "            data.to_excel(sub_path+\"/data/dataset.xlsx\", 'data', index=False)\n",
                "        \n",
                "        path = Path(sub_path+\"/data/labels.xlsx\")\n",
                "        if not path.is_file():\n",
                "            labels.to_excel(sub_path+\"/data/labels.xlsx\", 'data', index=False)\n",
                "        sub_path_list.append(sub_path)\n",
                "        progress = progress_manager.init_progress(sub_path, noise_algorithms, noise_levels, mlaas_providers)\n",
                "    return sub_path_list\n",
                "\n",
                "def run_evaluation(noise_levels_units: List[int],\n",
                "                   continue_from: str,    \n",
                "):\n",
                "    main_path = continue_from\n",
                "    progress = progress_manager.load_progress(main_path)\n",
                "\n",
                "    x_dataset = read_dataset(main_path + '/data/dataset.xlsx')\n",
                "    y_labels = read_dataset(main_path + '/data/labels.xlsx')\n",
                "\n",
                "    print('Generating noise...')\n",
                "    progress = noise_insertion.generate_noised_data(x_dataset, main_path, noise_package=noises)\n",
                "\n",
                "    print('Getting predictions from providers...')\n",
                "    progress = ml_providers.get_prediction_results(main_path)\n",
                "\n",
                "    print('Calculating metrics...')\n",
                "    metrics_results = metrics.metrics(progress, y_labels, main_path)\n",
                "\n",
                "    noise_list = [0]\n",
                "    noise_list.extend(noise_levels_units)\n",
                "\n",
                "    visualization.plot_results(metrics_results, main_path + '/results', noise_list, percent_noise=True)\n",
                "\n",
                "    print(main_path)\n",
                "\n",
                "timestamp = datetime.now().strftime(\"%m-%d-%Y %H_%M_%S\")\n",
                "\n",
                "path_list = prepare_execution(continue_from,\n",
                "                          timestamp, \n",
                "                          sample_size,\n",
                "                          word_counts,\n",
                "                          noise_algo,\n",
                "                          noise_level,\n",
                "                          [ml_providers.google, ml_providers.amazon, ml_providers.microsoft])\n",
                "for path in path_list:\n",
                "    run_evaluation(noise_level, \n",
                "                   continue_from=path)\n",
                "print(path_list)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "vscode": {
            "interpreter": {
                "hash": "98b75381e2c09cb53b84ee3c0ce4bb0c53f6c0b2bcf1d737d7d486e47dd4b47b"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}